{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapeo de Google\n",
    "\n",
    "## Descripción de Tareas\n",
    "\n",
    "Realizaremos scraping en el sitio web de Google Careers para obtener información sobre las vacantes de empleo.\n",
    "\n",
    "1. **Scraping URL (\"externalLink\") vacantes**\n",
    "   - Implementaremos una función para obtener las URLs de las vacantes.\n",
    "\n",
    "2. **Scraping información de cada vacante**\n",
    "   - Desarrollaremos una función para extraer información detallada de cada vacante.\n",
    "\n",
    "3. **Transformación de datos (Spoiler deben quedar 41 COLUMNAS)**\n",
    "   - Realizaremos la transformación de datos para asegurarnos de que haya 41 columnas en el resultado.\n",
    "\n",
    "4. **Diccionario con información de la compañía**\n",
    "   - Crearemos un diccionario con información específica de la compañía.\n",
    "\n",
    "## Configuración Inicial\n",
    "\n",
    "Realizaremos la configuración inicial del entorno y las importaciones necesarias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # Para analizar el HTML de las páginas web\n",
    "from pydantic import BaseModel \n",
    "from typing import List, Optional\n",
    "import pandas  as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL de la Página de Google Careers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, List, Optional\n",
    "from pydantic import BaseModel,Extra\n",
    "\n",
    "class Skill(BaseModel):\n",
    "    name: str\n",
    "    level: str\n",
    "    experience: int\n",
    "\n",
    "class Aptitude(BaseModel):\n",
    "    name: str\n",
    "\n",
    "class Tool(BaseModel):\n",
    "    name: str\n",
    "    level: str\n",
    "    experience: int\n",
    "\n",
    "class Language(BaseModel):\n",
    "    name: str\n",
    "    level: str\n",
    "\n",
    "class Benefit(BaseModel):\n",
    "    name: str\n",
    "\n",
    "class JobPost(BaseModel):\n",
    "    # id: Optional[int] = None\n",
    "    # companyId: Optional[str] = None\n",
    "    name: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    createdAt: Optional[str] = None\n",
    "    availableSlots: Optional[int] = None\n",
    "    skills: Optional[List[Skill]] = None\n",
    "    aptitudes: Optional[List[Aptitude]] = None\n",
    "    tools: Optional[List[Tool]] = None\n",
    "    languages: Optional[List[Language]] = None\n",
    "    benefits: Optional[List[Benefit]] = None\n",
    "    scholarity: Optional[str] = None\n",
    "    workhours: Optional[str] = None\n",
    "    locationConditions: Optional[str] = None\n",
    "    nationalRemote: Optional[Any] = None\n",
    "    minSalary: Optional[int] = None\n",
    "    maxSalary: Optional[int] = None\n",
    "    minAge: Optional[int] = None\n",
    "    maxAge: Optional[Any] = None\n",
    "    sex: Optional[str] = None\n",
    "    yearsOfExperience: Optional[int] = None\n",
    "    status: Optional[str] = None\n",
    "    country: Optional[str] = None\n",
    "    updatedAt: Optional[str] = None\n",
    "    driversLicense: Optional[bool] = None\n",
    "    degree: Optional[bool] = None\n",
    "    validPassport: Optional[bool] = None\n",
    "    validVisa: Optional[bool] = None\n",
    "    nationalRelocation: Optional[bool] = None\n",
    "    internationalRelocation: Optional[bool] = None\n",
    "    availabilityToTravel: Optional[bool] = None\n",
    "    seniority: Optional[str] = None\n",
    "    showSalaryRange: Optional[bool] = None\n",
    "    state: Optional[str] = None\n",
    "    city: Optional[str] = None\n",
    "    postalCode: Optional[str] = None\n",
    "    slug: Optional[str] = None\n",
    "    latitude: Optional[float] = None\n",
    "    longitude: Optional[float] = None\n",
    "    companyName: Optional[str] = None\n",
    "    companyImg: Optional[str] = None\n",
    "    class Config:\n",
    "        extra = Extra.ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Scraping URL de Vacantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página de Google Careers\n",
    "base_url = \"https://www.google.com/about/careers/applications/\"\n",
    "request_url = \"https://www.google.com/about/careers/applications/jobs/results/?utm_campaign=profilepage&utm_medium=profilepage&utm_source=linkedin&src=Online%2FLinkedIn%2Flinkedin_page&location=Mexico\"\n",
    "\n",
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code  == 200:\n",
    "        return BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_vacancy_urls():\n",
    "    soup = get_soup(request_url)\n",
    "    urls = []\n",
    "    \n",
    "    for posteo in soup.find_all('div', class_='sMn82b'):\n",
    "        name = posteo.text.strip()\n",
    "        link_tag = posteo.find('a')  # Buscar el elemento <a> dentro de posteo\n",
    "        if link_tag:\n",
    "            link = base_url + link_tag.get('href')  # Obtener el valor del atributo 'href'\n",
    "            urls.append({\"name\": name, \"link\": link})\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para guardar datos en formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    # Obtén la ruta completa del archivo en el directorio 'data'\n",
    "    file_path = os.path.join('data', filename)\n",
    "    \n",
    "    # Crea el directorio 'data' si no existe\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_vacantes = scrape_vacancy_urls()\n",
    "save_to_json(url_vacantes, 'url_vacantes.json')\n",
    "print(\"Vacantes guardadas en url_vacantes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Scraping información de cada vacante**\n",
    "   - Desarrollaremos una función para extraer información detallada de cada vacante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_vacancy_details(request_url):\n",
    "    soup = get_soup(request_url)\n",
    "    job_post = JobPost()\n",
    "\n",
    "    # Extraer información detallada de cada vacante\n",
    "    title_elem = soup.find(\"h2\", class_=\"p1N2lc\")\n",
    "    location_elem = soup.find(\"span\", class_=\"r0wTof\").find(\"span\", class_=\"pwO9Dc vo5qdf\")\n",
    "    requirements_elem = soup.find(\"div.KwJkGe > ul > li\")   \n",
    "    responsibilities_elem = soup.find(\"div\", class_=\"BDNOWe\").find(\"ul\").find(\"li\")\n",
    "\n",
    "    # Asignar valores a las propiedades del objeto JobPost\n",
    "    job_post.name = title_elem.text.strip() if title_elem else \"No disponible\"\n",
    "    job_post.locationConditions = location_elem.text.strip() if location_elem else \"No disponible\"\n",
    "    job_post.aptitudes = requirements_elem.text.strip() if requirements_elem else \"No disponible\"\n",
    "    job_post.skills = responsibilities_elem.text.strip() if responsibilities_elem else \"No disponible\"\n",
    "\n",
    "    # Convertir JobPost a json\n",
    "    formato_json = {\n",
    "        \"name\" : job_post.name,\n",
    "        \"location\" : job_post.locationConditions,\n",
    "        \"requirements\": job_post.aptitudes,\n",
    "        \"responsabilities\": job_post.skills\n",
    "    }\n",
    "\n",
    "    return formato_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para guardar datos en formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):    \n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detalles = []\n",
    "\n",
    "with open('data/url_vacantes.json', 'r') as archivo:\n",
    "    vacantes = json.load(archivo)\n",
    "\n",
    "# Iterar sobre la lista de diccionarios en el archivo JSON\n",
    "for vacante in vacantes:\n",
    "    # Obtener la URL de cada trabajo\n",
    "    url = vacante['link']\n",
    "    informacion_vacantes= scrape_vacancy_details(url)\n",
    "    \n",
    "    detalles.append(informacion_vacantes)\n",
    "\n",
    "save_to_json(detalles, 'data/informacion_vacantes.json')\n",
    "print(\"Vacantes guardadas en informacion_vacantes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Diccionario con información de la compañía**\n",
    "   - Crearemos un diccionario con información específica de la compañía.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diccionario_google = {\n",
    "    \"userId\": \"google\",\n",
    "    \"name\": \"Google\",\n",
    "    \"description\": \"\"\"\n",
    "                    Google es una empresa multinacional de tecnología estadounidense que se especializa en servicios y productos relacionados con Internet, incluyendo búsqueda en línea, publicidad, computación en la nube, software y hardware. \n",
    "                    Fundada en 1998, Google ha crecido hasta convertirse en una de las mayores empresas de tecnología del mundo. Su misión es organizar la información del mundo y hacerla universalmente accesible y útil.\n",
    "                    Google se esfuerza por crear tecnologías innovadoras que mejoren la vida de las personas y fomenten la accesibilidad a la información. La empresa valora la diversidad, la inclusión y el impacto positivo en la sociedad.\n",
    "                    \"\"\",\n",
    "    \"website\": \"https://www.google.com/\",\n",
    "    \"sector\": \"Tecnología\",\n",
    "    \"isExternal\": True,\n",
    "}\n",
    "\n",
    "# DataFrame de Google\n",
    "df_google = pd.DataFrame(diccionario_google, index=[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
